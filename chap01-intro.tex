\chapter{Introducción}
Cada día, utilizamos aplicaciones de cómputo más complejas que resuelven problemas más complicados. Por ejemplo, hay bancos que necesitan procesar millones de transacciones diariamente, así, utilizan aplicaciones que durante el día guardan las operaciones bancarias y en la noche ejecutan estas operaciones en lotes para actualizar las cuentas bancarias; en la Bolsa Mexicana de Valores, el motor de negociación transaccional, MoNeT, puede procesar hasta 100,000 transacciones por segundo \cite{bmv2012informe}. Otro ejemplo son las películas de animación; cuando los diseñadores han terminado de modelar a los personajes junto con el entorno y también han especificado las animaciones de los personajes sobre el entorno, se requiere genenar cada fotograma de la animación para después juntarlos y proyectarlos rápidamente para crear la ilusión de movimiento y, de esta forma, crear una película. Un último ejemplo son los proyectos de cómputo científico; éstos requieren hacer numerosos cálculos para llegar a resultados pertinentes, tal es el caso del descubrimiento del bosón de Higgs en el Gran Colisionador de Hadrones (LHC) del CERN; se estima que cada año, el detector principal del LHC genera 15 petabytes de datos que requieren ser analizados \cite{shiers2007worldwide}. 

¿Qué tienen en común todas estas aplicaciones? Para empezar, toman mucho tiempo en ejecutarse. Entonces, una posible solución para disminuir el tiempo de ejecución de estas aplicaciones es distribuir el gran trabajo que requieren estos proyectos entre varias computadoras. Para ello, necesitamos dividir nuestra aplicación en partes más pequeñas e independientes, algunas de ellas podrán ejecutarse de manera concurrente, otras no. De esta forma, tendríamos una solución escalable, es decir, si aumentamos el número de computadoras disponibles para nuestra aplicación, reduciremos el tiempo de ejecución.

Lograr esta paralelización requiere un esfuerzo por parte del desarrollador de la aplicación. Existen técnicas de paralelización que permiten al desarrollador expresar la aplicación en varias partes paralelas, por ejemplo: la administración multiproceso de los sistemas operativos, los threads en un proceso, el modelo MapReduce implementado en Hadoop, MPI implementado en OpenMPI. Aunque estas técnicas de paralelización son muy efectivas, éstas son aplicadas cuando el problema a resolver ha sido bien definido y cuando sólo hay una instancia del problema. Ahora bien, cuando la aplicación involucra varios pasos que están relacionados entre sí —por ejemplo, que el programa C requiera de la salida del programa A y del programa B para que pueda funcionar—, o cuando estamos definiendo los grandes bloques de solución del problema, hay una cierta secuencia que debemos seguir y, dentro de dicha secuencia, hay algunos pasos que podemos resolver de manera concurrente. Por lo tanto, estamos haciendo un \emph{modelo} de nuestro problema.

Este modelo también es llamado \emph{flujo de trabajo}. De manera muy abstracta, un flujo de trabajo es un conjunto de pasos que modelan la ejecución de un proceso. Este sencillo concepto ha sido aplicado en numerosas áreas, de las cuales mencionaremos dos ejemplos: en el ámbito de los negocios y en el ámbito de las ciencias en computación. 

En el ámbito de los negocios, se definen flujos de trabajos para modelar el proceso de fabricación de un producto. También, se pueden expresar flujos de trabajo con diagramas dibujados con los bloques y reglas del \emph{Lenguaje de Ejecución de Procesos de Negocio} para después simular el flujo de trabajo en una computadora.

En el ámbito de las ciencias en computación, los flujos de trabajo son aplicados para modelar programas que requieran varios pasos para su ejecución. Algunos flujos de trabajo requieren mucho tiempo tiempo de ejecución para sus pasos, como son el caso de los flujos de trabajo científicos. Otros flujos de trabajo son muy simples pero  se requieren que se ejecuten muchos de éstos. 

En cualquiera de los dos casos anteriores, es desable distribuir la ejecución de éstos flujos de trabajo entre varias computadoras. Si bien es posible paralelizar algunos pasos de la ejecución de nuestro flujo de trabajo utilizando las técnicas antes mencionadas, hay restricciones de orden que se deben respetar, por lo cual, se debe planear la ejecución del flujo de trabajo entre las múltiples computadoras.

%Hay varias maneras de hacer esta planificac. Cabe aclarar que
Definimos la \emph{planificación} como la asignación de recursos con el fin de completar la ejecución de todas las tareas de manera satisfactoria. Con ello, se desea encontrar una forma óptima de hacer esta planificación, como reducir el tiempo de ejecución total del flujo de trabajo. Sin embargo, con la aparición del cómputo en la nube, es posible ejecutar nuestro flujo de trabajo con otras restricciones, como minimizar el presupuesto necesario para la ejecución del flujo afectando el tiempo de ejecución.

%mencionar instruction level paralelism, data-level parallelism task level paralelism, 

%uml, bepl para software y negocios, buscar los que vienen en el paper de hierarchical scheduling for swindew-c

En esta tesis se hace un estudio de los principales algoritmos de planificación de flujos de trabajo, con énfasis en los algoritimos utilizados en cómputo distribuido, en especial en cómputo en la nube. En el capítulo 2 se habla de un estudio detallado del concepto de flujos de trabajo y su aplicación en computación. El capítulo 3 trata los principales enfoques de cómputo distribuido para ejecutar estos flujos. En el capítulo 4 se hace un estudio de los principales algoritmos de planificación de los flujos. Finalmente, en el capítulo 6 discutiremos algunas conclusiones sobre el análisis de estos algoritmos.
