\chapter{Introducción}
Cada día, utilizamos aplicaciones de cómputo más complejas que resuelven problemas más complicados. Por ejemplo, hay bancos que necesitan procesar millones de transacciones diariamente; así, utilizan aplicaciones que durante el día guardan las operaciones bancarias y en la noche ejecutan estas operaciones en lotes para actualizar las cuentas bancarias. En la Bolsa Mexicana de Valores, el motor de negociación transaccional, MoNeT, puede procesar hasta 100,000 transacciones por segundo \cite{bmv2012informe}. 

Otro ejemplo son las películas de animación: cuando los diseñadores han terminado de modelar a los personajes junto con el entorno y cuando también han especificado las animaciones de los personajes sobre el entorno, se requiere generar cada fotograma de la animación para después juntarlos y proyectarlos rápidamente para crear la ilusión de movimiento y, de esta forma, crear una película. Por ejemplo, en el 2005, investigadores de la Universidad de Innsbruck en Austria renderizaron una animación 3D, primero utilizando una sola computadora y luego utilizando varias computadoras interconectadas \cite{nerieri2005using}. En el primer caso, tardaron aproximadamente 6 días en procesar la animación cuya duración es de 1 minuto. Por otro lado, utilizando varias computadoras, la animación fue procesada en poco menos de una hora.

Un último ejemplo son los proyectos de cómputo científico; éstos requieren hacer numerosos cálculos para llegar a resultados pertinentes, tal es el caso del descubrimiento del bosón de Higgs en el Gran Colisionador de Hadrones (LHC) del CERN; se estima que cada año, el detector principal del LHC genera 15 petabytes (aproximadamente $15 \times 10^{15}$ bytes) de datos que requieren ser analizados \cite{shiers2007worldwide}. 

¿Qué tienen en común todas estas aplicaciones? Para empezar, toman mucho tiempo en ejecutarse. Entonces, una posible solución para disminuir el tiempo de ejecución de estas aplicaciones es distribuir el gran trabajo que requieren estos proyectos entre varias computadoras. Para ello, necesitamos dividir nuestra aplicación en partes más pequeñas e independientes, algunas de ellas podrán ejecutarse de manera concurrente, otras no. De esta forma, tendríamos una solución escalable, es decir, si aumentamos el número de computadoras disponibles para nuestra aplicación, reduciremos el tiempo de ejecución.

Lograr esta paralelización requiere un esfuerzo por parte del desarrollador de la aplicación. Existen técnicas de paralelización que permiten al desarrollador expresar la aplicación en varias partes paralelas. A continuación, enlistaremos algunas de estas técnicas:

\begin{itemize}
\item{\textbf{Programas multiproceso.} En esta técnica, la idea principal es utilizar varios procesos para repartir el cómputo. Por ejemplo, en el modelo fork/join, un programa se invoca recursivamente a sí mismo (fork), de tal modo que cada subprograma resuelve un problema más pequeño que el problema original. Después, los resultados parciales son juntados en uno sólo (join).}
\item{\textbf{Threads.} Un proceso, en vez de invocarse a sí mismo varias veces, también puede invocar threads o hilos de ejecución que, a diferencia de un proceso, éstos no son controlados por el administrador de procesos, sino por el programa mismo. Por lo tanto, ocupan menos recursos del sistema. De igual modo, se pueden implementar varios modelos de programación concurrente como se hacen con los programas multiproceso.}
\item{\textbf{MapReduce.} Este paradigma de programación especializado en procesar grandes volúmenes de datos funciona de la siguiente forma: primero de define una función $map(): (k_1,A) \rightarrow list(k_2,B)$, que se aplica a todos los elementos de tipo $A$ para asociarles una llave y transformarlos al tipo $B$. Luego, la función $reduce(): (k_2, list(B)) \rightarrow list(B)$, hace alguna operación para sumarizar los resultados.}
\item{\textbf{MPI.} La interfaz de paso de mensajes (MPI por sus siglas en inglés) es un conjunto de definiciones de bibliotecas en las que varios procesos paralelos pueden comunicarse entre si enviándose mensajes.}
\end{itemize}

Aunque estas técnicas de paralelización son muy efectivas, éstas son aplicadas cuando el problema a resolver ha sido bien definido y cuando sólo hay una instancia del problema. Ahora bien, cuando la aplicación involucra varios pasos que están relacionados entre sí —por ejemplo, que el programa $C$ requiera de la salida del programa $A$ y del programa $B$ para que pueda funcionar— o, cuando estamos definiendo los grandes bloques de solución del problema, hay una cierta secuencia que debemos seguir y, dentro de dicha secuencia, hay algunos pasos que podemos resolver de manera concurrente. Por lo tanto, estamos haciendo un \emph{modelo} de nuestro problema.

Este modelo también es llamado \emph{flujo de trabajo}. De manera muy abstracta, un flujo de trabajo es un conjunto de pasos que modelan la ejecución de un proceso. Este sencillo concepto ha sido aplicado en numerosas áreas, de las cuales mencionaremos dos ejemplos: en el ámbito de los negocios y en el ámbito científico. 

En el ámbito de los negocios, se definen flujos de trabajos para modelar el proceso de fabricación de un producto. También, se pueden expresar flujos de trabajo con diagramas dibujados con los bloques y reglas del \emph{Lenguaje de Ejecución de Procesos de Negocio} para después simular el flujo de trabajo en una computadora.

En el ámbito científico, los flujos de trabajo son aplicados para modelar programas que requieran varios pasos para su ejecución. Algunos flujos de trabajo requieren mucho tiempo tiempo de ejecución para sus pasos, como son el caso de los flujos de trabajo científicos. Otros flujos de trabajo son muy simples pero se requieren que se ejecuten muchos de éstos. 

En cualquiera de los dos casos anteriores, es deseable distribuir la ejecución de éstos flujos de trabajo entre varias computadoras. Si bien es posible paralelizar algunos pasos de la ejecución de nuestro flujo de trabajo utilizando las técnicas antes mencionadas, hay restricciones de orden que se deben respetar, por lo cual, se debe planear la ejecución del flujo de trabajo entre las múltiples computadoras.

%Hay varias maneras de hacer esta planificac. Cabe aclarar que
Definimos la \emph{planificación} como la asignación de recursos con el fin de completar la ejecución de todas las tareas de manera satisfactoria. Con ello, se desea encontrar una forma óptima de hacer esta planificación, como reducir el tiempo de ejecución total del flujo de trabajo. Sin embargo, con la aparición del cómputo en la nube, es posible ejecutar nuestro flujo de trabajo con otras restricciones, como minimizar el presupuesto necesario para la ejecución del flujo afectando el tiempo de ejecución.

%mencionar instruction level paralelism, data-level parallelism task level paralelism, 

%uml, bepl para software y negocios, buscar los que vienen en el paper de hierarchical scheduling for swindew-c

En esta tesis se hace un estudio de los principales algoritmos de planificación de flujos de trabajo, con énfasis en los algoritmos utilizados en cómputo distribuido, en especial en cómputo en la nube. En el capítulo 2 se habla de un estudio detallado del concepto de flujos de trabajo y su aplicación en computación. El capítulo 3 trata los principales enfoques de cómputo distribuido para ejecutar estos flujos. En el capítulo 4 se hace un estudio de los principales algoritmos de planificación de los flujos. Finalmente, en el capítulo 6 discutiremos algunas conclusiones sobre el análisis de estos algoritmos.
